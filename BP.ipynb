{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn import datasets\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "digits = datasets.load_digits()\n",
    "np.random.seed(1222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def init_params(n_input, n_hidden, n_output):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    \n",
    "        n_input : size of input layer\n",
    "        n_output : size of output layer\n",
    "        n_hidden : size of hidden layer\n",
    "        \n",
    "        W1 : weight from input to hidden layer\n",
    "        b1 : bias from input to hidden layer\n",
    "        W2 : weight from hidden to output layer\n",
    "        b2 : bias from hidden to output layer\n",
    " \n",
    "    Return:\n",
    "        \n",
    "        Dictionary type {W1,b1,W2,b2}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = np.random.randn(n_hidden, n_input) * 0.01\n",
    "    b1 = np.zeros((n_hidden,1))\n",
    "    W2 = np.random.randn(n_output, n_hidden) * 0.01\n",
    "    b2 = np.zeros((n_output,1))\n",
    "    \n",
    "    params = {'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2}\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, params):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        \n",
    "        x :  training data\n",
    "        param : containging W1,b1,W2,b2\n",
    "        \n",
    "    Return:\n",
    "    \n",
    "        cache : store the input and ouput of hidden_layer and output_layer\n",
    "    \n",
    "    \"\"\"\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1,x) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {'Z1':Z1,'A1':A1,'Z2':Z2,'A2':A2}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(lambda_, params):\n",
    "    return lambda_ * (np.sum(np.power(params['W1'],2)) +  np.sum(np.power(params['W2'],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(A2, y, batch, lambda_, params):\n",
    "    \"\"\"\n",
    "    Params : \n",
    "        y : training label\n",
    "        A2 : output prob of output layer\n",
    "        \n",
    "    Return :\n",
    "        cost : cross entropy\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    loss = np.multiply(np.log(A2), y) + np.multiply(np.log(1-A2), (1-y))\n",
    "    cost = -(1.0 / batch) * np.sum(loss) + regularization(lambda_,params)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y, params, cache, batch):\n",
    "    \"\"\"\n",
    "    Params : \n",
    "        x :  training data feature\n",
    "        y : training data label\n",
    "        \n",
    "        param : containging W1,b1,W2,b2\n",
    "        cache : store the input and ouput of hidden_layer and output_layer\n",
    "        \n",
    "    Return : \n",
    "        grad : gradient descent from back to front\n",
    "    \"\"\"\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z1 = cache['Z1']\n",
    "    Z2 = cache['Z2']\n",
    "    \n",
    "    \n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "    \n",
    "    dZ2 = (A2 - y)\n",
    "    dW2 = (1.0 / batch) * np.dot(dZ2,A1.T)\n",
    "    db2 = (1.0 / batch) * np.sum(dZ2,axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1-np.power(A1,2))\n",
    "    dW1 = (1.0 / batch) * np.dot(dZ1, x.T)\n",
    "    db1 = (1.0 / batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grad = {'dW1' : dW1,\n",
    "           'db1' : db1,\n",
    "           'dW2': dW2,\n",
    "           'db2' : db2}\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params, grad, lr):\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "    dW1 = grad['dW1']\n",
    "    db1 = grad['db1']\n",
    "    dW2 = grad['dW2']\n",
    "    db2 = grad['db2']\n",
    "    \n",
    "    W1 = W1 - lr * dW1\n",
    "    b1 = b1 - lr * db1\n",
    "    W2 = W2 - lr * dW2\n",
    "    b2 = b2 - lr * db2\n",
    "    param =  {'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2}\n",
    "    \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x_test,y_test,params,threshold):\n",
    "    pred = forward(x_test, params)['A2']\n",
    "    pred[pred>=threshold] = 1\n",
    "    pred[pred<threshold] = 0\n",
    "\n",
    "    return np.sum(pred==y_test) / x_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, x_val,y_val, n_hidden, classes, lr,threshold, lambda_):\n",
    "    n_input = x.shape[0]\n",
    "    batch = x.shape[1]\n",
    "    params = init_params(n_input, n_hidden, classes)\n",
    "    for i in range(20000):\n",
    "        cache = forward(x,params)\n",
    "        cost = cross_entropy(cache['A2'], y,batch, lambda_, params)\n",
    "        grad = backprop(x, y, params, cache, batch)\n",
    "        params = update(params, grad, lr)\n",
    "        if i % 2000 == 0:\n",
    "            pred = evaluate(x_val, y_val, params, threshold)\n",
    "            print(\"Accuracy: %.4f, loss: %.6f\" % (pred, cost))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_X = digits.data\n",
    "data_train_Y = digits.target\n",
    "choose = data_train_Y >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_train_X[choose].T\n",
    "y = data_train_Y[choose]-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8000, loss: 0.693115\n",
      "Accuracy: 1.0000, loss: 0.099347\n",
      "Accuracy: 1.0000, loss: 0.036996\n",
      "Accuracy: 1.0000, loss: 0.021731\n",
      "Accuracy: 1.0000, loss: 0.014955\n",
      "Accuracy: 1.0000, loss: 0.011244\n",
      "Accuracy: 1.0000, loss: 0.008938\n",
      "Accuracy: 1.0000, loss: 0.007379\n",
      "Accuracy: 1.0000, loss: 0.006261\n",
      "Accuracy: 1.0000, loss: 0.005423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_hidden = 20;\n",
    "classes = 1;\n",
    "lr = 1e-3\n",
    "threshold = 0.5\n",
    "test_index = 5\n",
    "\n",
    "xtest = x[:,:test_index]\n",
    "ytest = y[:test_index]\n",
    "xtrain = x[:,test_index:]\n",
    "ytrain = y[test_index:]\n",
    "\n",
    "train(xtrain, ytrain, xtest, ytest, n_hidden, classes, lr, threshold, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
